{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f5e6df-181b-40fd-9b6b-ead90e343de4",
   "metadata": {},
   "source": [
    "# Decesion Tree from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbc630-d948-4567-a2de-71037907830f",
   "metadata": {},
   "source": [
    "### Assumption to consider\n",
    "\n",
    "1. This decesion tree only works well with multi-class classfication problems\n",
    "2. Decesion tree build during training will always be binary tree\n",
    "3. Works for numerical and categorical data (but not optimised for numeric data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128cbaef-c49c-456a-8040-fcebca2fd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e43a9-21d5-4685-b7a7-5625c360cde3",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "$$ H(p) = -\\sum_{i=0}^{c} p_c\\log{(p_c)}$$\n",
    "\n",
    "##### where $ \\sum_{i=0}^{c} p_c = 1 $ #####\n",
    "\n",
    "##### $p_c$ is the frequency of the every ith category #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2c1c75-9ded-47ce-84c7-7dc94a84e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my entropy: 0.5623351446188083\n"
     ]
    }
   ],
   "source": [
    "def entropy(y):\n",
    "    '''\n",
    "    Calculates impurities in the target value \n",
    "\n",
    "    Args:       \n",
    "        y ndarray(m, )\n",
    "\n",
    "    Returns:\n",
    "        entropy (scalar)\n",
    "    '''\n",
    "    categories, cnt = np.unique(y, return_counts = True)\n",
    "    p = cnt / len(y)\n",
    "\n",
    "    entropy = -np.sum(p * np.log(p))\n",
    "    return entropy\n",
    "\n",
    "# test \n",
    "y_temp = np.array([1,1, 0, 1])\n",
    "print(f\"my entropy: {entropy(y_temp)}\") # o/p 0.5623351446188083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb398dc-8797-480b-9fbd-6d975ccebe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2]), array([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(X, feature, threshold):\n",
    "    '''\n",
    "    Returns indices of left split and right split. This is done based on feature and threshold given.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray(m, n))\n",
    "        feature (scalar)\n",
    "        threshold (scalar)\n",
    "        \n",
    "    Returns:\n",
    "        left_idxs (ndarray(m2))\n",
    "        right_idxs (ndarray(m2,))\n",
    "    '''\n",
    "    left_idxs = np.argwhere(X[:, feature] <= threshold).flatten()\n",
    "    right_idxs = np.argwhere(X[:, feature] > threshold).flatten()\n",
    "\n",
    "    return left_idxs, right_idxs\n",
    "\n",
    "# test\n",
    "x_temp = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "])\n",
    "split(x_temp, 2, 0) # o/p: (array([1, 2]), array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ecfe5-6663-49db-bbef-f5c4680ad483",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "$$ \\text{infomation gain} = H(Parent) - H_w(Children) $$\n",
    "\n",
    "$$ \\text{where  } H_w(Children) = w_{left}*H(left) + w_{right}*H(right)$$ \n",
    "\n",
    "#### here __H__ is entropy\n",
    "#### $H_w$ is weighted entropy\n",
    "#### $w_{left} = \\frac{\\text{No. of datapoints in left node}}{\\text{Total no. of datapoints in parent node}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81aa47ef-abd3-4df3-9465-ecf8995fa410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: 0, info_gain: 0.17441604792151594\n",
      "feature: 1, info_gain: 0.6365141682948128\n",
      "feature: 2, info_gain: 0.6365141682948128\n"
     ]
    }
   ],
   "source": [
    "def information_gain(X, y, feature, threshold):\n",
    "    '''\n",
    "    Args:\n",
    "        X (ndarray(m, n))\n",
    "        y (ndarray(m,))\n",
    "        feature (scalar)\n",
    "        threshold (scalar)\n",
    "\n",
    "    Returns:\n",
    "        info_gain (scalar)\n",
    "    '''\n",
    "    # split data into left and right idices\n",
    "    left_idxs, right_idxs = split(X, feature, threshold)\n",
    "    \n",
    "    # Calculate entropy of parent and children node\n",
    "\n",
    "    w_left = len(left_idxs) / len(y)\n",
    "    w_right = len(right_idxs) / len(y)\n",
    "    entropy_left = entropy(y[left_idxs])\n",
    "    entropy_right = entropy(y[right_idxs])\n",
    "    \n",
    "    entropy_parent = entropy(y)\n",
    "    entropy_children = w_left * entropy_left + w_right * entropy_right\n",
    "    \n",
    "    # calculate information gain\n",
    "    info_gain =  entropy_parent - entropy_children\n",
    "\n",
    "    return info_gain\n",
    "\n",
    "# test\n",
    "temp_X = np.array([\n",
    "    [1,0,1], \n",
    "    [1,1,0], \n",
    "    [0,1,0]\n",
    "])\n",
    "temp_y = np.array([1,0,0])\n",
    "\n",
    "temp_node_indices = np.array([0,1,2])\n",
    "temp_feature = [0,1,2]\n",
    "\n",
    "for f in temp_feature:\n",
    "    info_gain = information_gain(temp_X, temp_y, f, 0)\n",
    "    print(f\"feature: {f}, info_gain: {info_gain}\")\n",
    "    \n",
    "## output \n",
    "# feature: 0, info_gain: 0.17441604792151594\n",
    "# feature: 1, info_gain: 0.6365141682948128\n",
    "# feature: 2, info_gain: 0.6365141682948128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7dc53c-487c-4e26-b775-a1a6b0f80f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best feature: 1, best thresold: 0\n"
     ]
    }
   ],
   "source": [
    "def best_split(X, y):\n",
    "    '''\n",
    "    Returns the feature and threshold that is hightest information w.r.t given datapoints\n",
    "\n",
    "    Args:\n",
    "        X (ndarray(m,n))\n",
    "        y (ndarray(m,))\n",
    "    Returns:\n",
    "        best_feature (scalar): index of the feature\n",
    "        best_threshold (scalar): one of the category or number from the feature\n",
    "    '''\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    best_feature = -1\n",
    "    best_threshold = -1\n",
    "    best_info_gain = -1\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        categories = np.unique(X[:, feature])\n",
    "        \n",
    "        for threshold in categories:\n",
    "            info_gain = information_gain(X, y, feature, threshold)\n",
    "\n",
    "            if best_info_gain < info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_threshold = threshold\n",
    "                best_feature = feature\n",
    "                \n",
    "    return best_feature, best_threshold\n",
    "\n",
    "# test\n",
    "temp_X = np.array([\n",
    "    [1,0,1], \n",
    "    [1,1,0], \n",
    "    [0,1,0]\n",
    "])\n",
    "\n",
    "temp_y = np.array([1,0,0])\n",
    "temp_node_indices = np.array([0,1,2])\n",
    "temp_feature = [0,1,2]\n",
    "\n",
    "temp_best_feature, temp_best_threshold = best_split(temp_X, temp_y)\n",
    "print(f\"best feature: {temp_best_feature}, best thresold: {temp_best_threshold}\") \n",
    "\n",
    "## Output\n",
    "#  best feature: 1, best thresold: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b292aa8-4fc7-4c44-be94-ffa60161e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, predicted_value=None):\n",
    "        '''\n",
    "        feature (scalar): Index of feature. On basis of this feature the node should split data.\n",
    "        threshold (scalar): Value of the feature on from which other datapoint is compared for spliting \n",
    "        predicted_value (scalar): If node is leaf node in decesion then it will return prediction value. Otherwise it will be null.\n",
    "        left (Node)\n",
    "        right (Node)\n",
    "        '''\n",
    "\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.predicted_value = predicted_value\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.predicted_value != None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710669ac-4fe6-4dfd-968f-a729f1ac0bd4",
   "metadata": {},
   "source": [
    "## Base case in Build Tree \n",
    "Return leaf node if:\n",
    "1. current node depth is equal to max_depth.\n",
    "2. all given examples' target value is same.\n",
    "3. number of examples are less than nnumber of splits we make of a child node(i.e 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc0c68a-aad2-4d9b-a098-ec6bd75ed933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, curr_depth ,max_depth):\n",
    "        '''\n",
    "        It creates decesion tree based on given training data\n",
    "\n",
    "        Args:\n",
    "            X (ndarray(m, n)): m examples and n features\n",
    "            y (ndarray(m, )): target value of m examples\n",
    "            max_depth (scalar)\n",
    "        Return:\n",
    "            root (Node): return parent node\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "    \n",
    "        # base case\n",
    "        if curr_depth == max_depth or len(np.unique(y)) == 1 or m < 2:\n",
    "            # find most common label\n",
    "            unique_labels, cnt = np.unique(y, return_counts=True)\n",
    "            max_cnt_idx = np.argmax(cnt)\n",
    "            most_common_label =  unique_labels[max_cnt_idx]\n",
    "\n",
    "            return Node(predicted_value=most_common_label)\n",
    "            \n",
    "        # select best feature and threshold \n",
    "        feature, threshold = best_split(X, y)\n",
    "    \n",
    "        # split data based on best feature and threshold\n",
    "        left_idxs, right_idxs = split(X, y, feature, threshold)\n",
    "    \n",
    "        # call child node\n",
    "        left_node = build_tree(X[left_idxs], y[left_idxs], curr_depth+1, max_depth)\n",
    "        right_node = build_tree(X[right_idxs], y[right_idxs], curr_depth+1, max_depth)\n",
    "\n",
    "        return Node(feature, threshold, left_node, right_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969b68a6-f95c-498c-b6a6-924723907b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100):\n",
    "        self.max_depth = 100\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = build_tree(X, y, curr_depth=0, max_depth=self.max_depth)\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        return np.sum(y_pred == y) / len(y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if not self.root:\n",
    "            raise ValueError(\"The tree has not been trained yet.\")\n",
    "            \n",
    "        ans = np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        return ans\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        '''Predicts target value for single datapoint'''\n",
    "        if node.is_leaf_node():\n",
    "            return node.predicted_value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "            \n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbc9e4",
   "metadata": {},
   "source": [
    "### Test decesion tree works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e058e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from DecisionTree import DecisionTree\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {acc: .3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
