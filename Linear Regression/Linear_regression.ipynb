{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524f02d1-2114-4fc3-965b-a37cb07d0a1d",
   "metadata": {},
   "source": [
    "# Multivariable linear regression \n",
    "\n",
    "__Assumptions__\n",
    "1. We are not using any type of regularisation technique.\n",
    "2. For formulas I used one-based indexing while in code we are using zero-based indexing\n",
    "\n",
    "__Terminologies__\n",
    "1. m represents no. of examples(rows) and n represents no. of features(columns).\n",
    "2. $w_j$ is coefficient of jth feature($x_j$) and b is constant in the model(equation)\n",
    "3. $\\vec{w}$ vector represents all coefficients of the model.\n",
    "4. $\\vec{x}$ vector represents the single row with all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1684fdaa-c3db-4b15-9686-050617c2aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48692017-d028-4751-b843-c70dcb256306",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "$$ \\hat{y} = f_{\\vec{w}, b}(\\vec{x}) =  \\sum_{j=1}^{n} (w_jx_j) + b$$\n",
    "\n",
    "\n",
    "$$ \\text{or}$$\n",
    "\n",
    "$$ \\hat{y} = f_{\\vec{w}, b}(\\vec{x})  =\\vec{w}. \\vec{x} + b $$\n",
    "\n",
    "here $n$ is no. of features \\\n",
    "$\\vec{w}$ and $b$ are model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8613abee-5631-4917-839f-8eaa05a1a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(X, w, b):\n",
    "    '''\n",
    "    Computes the output of the model when given model parameters and inputs\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    X (ndarray(m, n)): m examples with n features\n",
    "    w (ndarray(n,)): model parameters(n weights for n features).\n",
    "    b (scalar): model parameter.\n",
    "\n",
    "    Returns:\n",
    "        f_wb (ndarray(m, )): m predictions for m input examples\n",
    "    '''\n",
    "\n",
    "    f_wb = np.dot(X, w.T) + b\n",
    "\n",
    "    return f_wb\n",
    "\n",
    "# test\n",
    "X_temp = np.array([[1,2,3,4], [1,2,3,4]])\n",
    "w_temp = np.array([1,2,3,4])\n",
    "b_temp = 0\n",
    "\n",
    "predict(X_temp, w_temp, b_temp) #array([30, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79832e46-2046-498e-a273-568cf885275d",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "For training purpose, _Squared error_ is used\n",
    "\n",
    "$$ J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "$$ \\text{or} $$\n",
    "$$ J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)})- y^{(i)})^2 $$\n",
    "\n",
    "$$ \\text{or} $$\n",
    "\n",
    "$$ J(\\vec{w}, b) =  \\frac{1}{2m} \\sum_{i=1}^{m}(\\vec{w}.\\vec{x}^{(i)} + b - y^{(i)})^2$$\n",
    "\n",
    "where $m$ is number of training exapmles.\\\n",
    "$i$ represents the ith example not __power__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53b68ae8-c67d-4867-991f-9ef12ef62838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_squared_error(X, y, w, b):\n",
    "    '''\n",
    "    Returns squared error when given input, output and model parameters\n",
    "\n",
    "    Args:\n",
    "        X (ndarray(m, n)): m examples and n features\n",
    "        y (ndarray(m)): m outputs for m examples\n",
    "        w (ndarray(n, )): model parameter\n",
    "        b (scalar): model parameter\n",
    "\n",
    "    Returns:\n",
    "        cost (scalar): Squared error\n",
    "    '''\n",
    "    m = X.shape[0]  #  m is no of examples in X\n",
    "    f_wb = np.dot(X, w.T) + b\n",
    "    error = (f_wb - y)**2\n",
    "    cost = np.sum(error) / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "# test\n",
    "X_temp = np.array(\n",
    "    [\n",
    "        [1,2,3,4], \n",
    "        [1,2,3,4]\n",
    "    ])\n",
    "y_temp = [1,1]\n",
    "w_temp = np.array([1,2,3,4])\n",
    "b_temp = 0\n",
    "\n",
    "compute_squared_error(X_temp, y_temp, w_temp, b_temp) # 420.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c2dfd-7a8a-4c70-b23e-e673335eef76",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "The gradient is used to know the slope of the cost function at at some $\\vec{w} \\text{ and } b$. It tell us in which direction to move the parameter such that cost fucntion become minimal.\n",
    "\n",
    "$$ \\frac{\\partial{J(\\vec{w}, b)}}{\\partial{w_j}} = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})x_j$$\n",
    "$$ \\frac{\\partial{J(\\vec{w}, b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "For each paramerter the gradient is determined \n",
    "\n",
    "here $\\frac{\\partial{J(\\vec{w}, b)}}{\\partial{w_j}}$ is gradient wrt weight of jth feature  \n",
    "\n",
    "$\\frac{\\partial{J(\\vec{w}, b)}}{\\partial{b}}$ is gradient wrt bias(constant without feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1040a845-3b16-4fef-b8d3-552f558b21b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 29.,  58.,  87., 116.]), 29.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    '''\n",
    "    returns partial differenciation of cost function w.r.t all model parameters(w, b)\n",
    "\n",
    "    Args:\n",
    "        X (ndarray(m, n))\n",
    "        y (ndarray(m))\n",
    "        w (ndarray(m))\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    f_wb = np.dot(X,w) + b\n",
    "    error = f_wb - y\n",
    "    dJ_dw = np.dot(X.T, error) / m\n",
    "    dJ_db = np.sum(error) / m\n",
    "    \n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "# test\n",
    "X_temp = np.array(\n",
    "    [\n",
    "        [1,2,3,4], \n",
    "        [1,2,3,4]\n",
    "    ])\n",
    "y_temp = [1,1]\n",
    "w_temp = np.array([1,2,3,4])\n",
    "b_temp = 0\n",
    "compute_gradient(X_temp, y_temp, w_temp, b_temp) # (array([ 29.,  58.,  87., 116.]), 29.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739386e-7d45-47af-9e35-3093aad024e1",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- Gradient descent is also called _Update rule_\n",
    "- It tells in which direction to modify parameters to make squared error(cost) of the model minimal.\n",
    " \n",
    "$$ w_j = w_j - \\alpha \\frac{\\partial{J(\\vec{w}, b)}}{\\partial{w_j}} $$\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial{J(\\vec{w}, b)}}{\\partial{b}} $$\n",
    "\n",
    "where $j$ belongs from 1 to n \\\n",
    "$\\alpha$ is learning rate. It controls to magnitude with which model parameters should update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb0d0554-8754-4c9e-961d-f6737b89d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iterations=1000, *, alpha=0.01, w_in=None, b_in=None):\n",
    "    '''\n",
    "    Returns final model parameters(w, b) after training\n",
    "\n",
    "    Args:\n",
    "        X (ndarray(m, n))\n",
    "        y (ndarray(m,))\n",
    "        iterations (scalar): no of times gardient decesnt is applied to update w and b to minise cost\n",
    "        alpha (scalar): It represents learning rate\n",
    "        w_in (ndarray(n,)): initial parameter value(optional)\n",
    "        b_in (scalar): initial parameter value(optiona)\n",
    "\n",
    "    Return:\n",
    "        w (ndarray(n,)): model paramerter after training\n",
    "        b (scalar): model paramerter after training\n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    \n",
    "    w = w_in if w_in != None else np.zeros(n)\n",
    "    b = b_in if b_in != None else 0.0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        dJ_dw, dJ_db = compute_gradient(X, y, w, b)\n",
    "        if np.isnan(w).any() or np.isnan(dJ_dw).any():\n",
    "            print(\"NaN encountered\")\n",
    "            print(f\"w: {w}\")\n",
    "            print(f\"dJ_dw: {dJ_dw}\")\n",
    "        if np.isinf(w).any() or np.isinf(dJ_dw).any():\n",
    "            print(\"Infinity encountered\")\n",
    "            print(f\"w: {w}\")\n",
    "            print(f\"dJ_dw: {dJ_dw}\")\n",
    "\n",
    "        w = w - alpha*dJ_dw\n",
    "        b = b - alpha*dJ_db\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3935dd11-e217-4900-9496-b9a3ef1e8a4d",
   "metadata": {},
   "source": [
    "## Let us test gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07ce86ee-aabf-4f6d-9a60-73dad33bbeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error for training data: 0.2756857061752969\n",
      "Squared error for test data: 0.27755306514461875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, random_state=42)\n",
    "w_final, b_final = gradient_descent(X_train, y_train)\n",
    "\n",
    "print(f\"Squared error for training data: {compute_squared_error(X_train, y_train, w_final, b_final)}\")\n",
    "print(f\"Squared error for test data: {compute_squared_error(X_test, y_test, w_final, b_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042b725-e051-411e-b00c-c5248dc85df3",
   "metadata": {},
   "source": [
    "#### fit() function is same as gradient_descent(), only differnce is it don't dependent on other fucntion(such as compute_gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d7ebbcd-bd0e-47b7-aaa7-63d1ce8dac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error for training data: 0.260276475430372\n",
      "Squared error for test data: 0.2706067903150232\n"
     ]
    }
   ],
   "source": [
    "def fit(X, y, iterations=10000, *, alpha=0.01, w_in=None, b_in=None):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    w = w_in if w_in else np.zeros(n)\n",
    "    b = b_in if b_in else 0.0\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        f_wb = np.dot(X, w.T) + b\n",
    "        error = f_wb - y\n",
    "\n",
    "        dJ_dw = np.matmul(X.T, error) / m\n",
    "        dJ_db = np.sum(error) / m\n",
    "\n",
    "        w = w - alpha*dJ_dw\n",
    "        b = b - alpha*dJ_db\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# test\n",
    "w_final, b_final = fit(X_train, y_train)\n",
    "\n",
    "print(f\"Squared error for training data: {compute_squared_error(X_train, y_train, w_final, b_final)}\")\n",
    "print(f\"Squared error for test data: {compute_squared_error(X_test, y_test, w_final, b_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfdfc02d-b6b6-4332-8af6-b49d5022e955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Squared error for training data: 0.26027610818225644\n",
      "Expecteed Squared error for test data: 0.27056437392353444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "w_final, b_final = reg.coef_, reg.intercept_\n",
    "\n",
    "print(f\"Expected Squared error for training data: {compute_squared_error(X_train, y_train, w_final, b_final)}\")\n",
    "print(f\"Expecteed Squared error for test data: {compute_squared_error(X_test, y_test, w_final, b_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3f9ee-7b76-404d-9f95-a4821a1113ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
